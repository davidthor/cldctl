---
title: "Database Hook"
description: "Provision databases for components"
---

# Database Hook

The database hook provisions databases when components declare database requirements. It receives the database specification and must return connection information.

## Basic Usage

```hcl
database {
  when = element(split(":", node.inputs.type), 0) == "postgres"

  module "db" {
    build = "./modules/rds-postgres"
    inputs = {
      name    = "${environment.name}-${node.component}-${node.name}"
      version = try(element(split(":", node.inputs.type), 1), null)
      region  = variable.region
    }
  }

  outputs = {
    host     = module.db.host
    port     = module.db.port
    database = module.db.database
    username = module.db.username
    password = module.db.password
    url      = module.db.connection_url
  }
}
```

## Inputs

The following inputs are available via `node.inputs`:

| Field | Type | Description |
|-------|------|-------------|
| `name` | string | Resource identifier |
| `type` | string | Database type and version (e.g., postgres:^16, redis:7) |
| `migrations` | object | Optional migration configuration |

## Required Outputs

| Field | Type | Description |
|-------|------|-------------|
| `host` | string | Database hostname |
| `port` | number | Database port |
| `database` | string | Database name |
| `url` | string | Full connection URL |

Optional outputs:

| Field | Type | Description |
|-------|------|-------------|
| `username` | string | Database username |
| `password` | string | Database password |
| `read` | object | Optional read endpoint (nested object with `host`, `port`, `url`, `username`, `password`) |
| `write` | object | Optional write endpoint (nested object with `host`, `port`, `url`, `username`, `password`) |

### Read/Write Endpoint Separation

Database hooks can optionally provide separate `read` and `write` sub-objects within their outputs. This enables advanced infrastructure workflows like zero-downtime database migrations where read and write traffic temporarily route to different backends.

**Fallback behavior**: If the operator omits `read` and/or `write` from the outputs block, the engine automatically populates them by mirroring the top-level flat outputs. Datacenter authors never need to declare `read`/`write` unless they want them to differ from the root values. This means **zero changes** to existing datacenters.

```hcl
database {
  when = element(split(":", node.inputs.type), 0) == "postgres"

  module "primary" {
    build = "./modules/rds-postgres"
    inputs = {
      name = "${environment.name}-${node.component}-${node.name}"
    }
  }

  outputs = {
    # Standard flat outputs (unchanged, backwards compatible)
    host     = module.primary.host
    port     = module.primary.port
    url      = module.primary.url
    username = module.primary.username
    password = module.primary.password
    database = node.name

    # Optional nested read/write objects
    read = {
      host     = module.primary.host
      url      = module.primary.url
      username = module.primary.username
      password = module.primary.password
    }

    write = {
      host     = module.primary.host
      url      = module.primary.url
      username = module.primary.username
      password = module.primary.password
    }
  }
}
```

## Multiple Database Types

You can define multiple `database` hooks with different `when` conditions to handle different database engines. Hooks are evaluated **top-to-bottom in source order**, and **only the first matching hook is executed** for each resource (waterfall-style). This means order matters -- put more specific hooks before broader ones:

```hcl
# PostgreSQL databases
database {
  when = element(split(":", node.inputs.type), 0) == "postgres"

  module "postgres" {
    build = "./modules/rds-postgres"
    inputs = {
      name    = "${environment.name}-${node.component}-${node.name}"
      version = try(element(split(":", node.inputs.type), 1), null)
    }
  }

  outputs = {
    host     = module.postgres.endpoint
    port     = module.postgres.port
    database = module.postgres.database_name
    url      = module.postgres.connection_url
  }
}

# MySQL databases
database {
  when = element(split(":", node.inputs.type), 0) == "mysql"

  module "mysql" {
    build = "./modules/rds-mysql"
    inputs = {
      name    = "${environment.name}-${node.component}-${node.name}"
      version = try(element(split(":", node.inputs.type), 1), null)
    }
  }

  outputs = {
    host     = module.mysql.endpoint
    port     = module.mysql.port
    database = module.mysql.database_name
    url      = module.mysql.connection_url
  }
}

# Redis databases
database {
  when = element(split(":", node.inputs.type), 0) == "redis"

  module "redis" {
    build = "./modules/elasticache-redis"
    inputs = {
      name    = "${environment.name}-${node.component}-${node.name}"
      version = try(element(split(":", node.inputs.type), 1), null)
    }
  }

  outputs = {
    host = module.redis.endpoint
    port = module.redis.port
    url  = "redis://${module.redis.endpoint}:${module.redis.port}"
  }
}

# Reject anything else with a helpful message
database {
  error = "Unsupported database type '${node.inputs.type}'. This datacenter supports: postgres, mysql, redis."
}
```

The final `database` hook has no `when` condition, so it acts as a catch-all. If a component requests a database type that isn't handled above (e.g., MongoDB), the deployment is blocked with the error message. See [Error Handling](/datacenters/error-handling) for more details.

## Example Pulumi Module

Here's an example Pulumi module for provisioning RDS PostgreSQL:

```typescript
// modules/rds-postgres/index.ts
import * as pulumi from "@pulumi/pulumi";
import * as aws from "@pulumi/aws";

const config = new pulumi.Config();
const name = config.require("name");
const version = config.get("version") || "15";

const db = new aws.rds.Instance("database", {
  identifier: name,
  engine: "postgres",
  engineVersion: version,
  instanceClass: "db.t3.micro",
  allocatedStorage: 20,
  dbName: "main",
  username: "postgres",
  password: pulumi.secret("generated-password"),
  skipFinalSnapshot: true,
});

export const endpoint = db.endpoint;
export const port = db.port;
export const database_name = db.dbName;
export const connection_url = pulumi.interpolate`postgresql://${db.username}:${db.password}@${db.endpoint}/${db.dbName}`;
```

## Example OpenTofu Module

```hcl
# modules/rds-postgres/main.tf
variable "name" {
  type = string
}

variable "version" {
  type    = string
  default = "15"
}

resource "aws_db_instance" "database" {
  identifier           = var.name
  engine               = "postgres"
  engine_version       = var.version
  instance_class       = "db.t3.micro"
  allocated_storage    = 20
  db_name              = "main"
  username             = "postgres"
  password             = random_password.db_password.result
  skip_final_snapshot  = true
}

resource "random_password" "db_password" {
  length  = 32
  special = false
}

output "endpoint" {
  value = aws_db_instance.database.endpoint
}

output "port" {
  value = aws_db_instance.database.port
}

output "database_name" {
  value = aws_db_instance.database.db_name
}

output "connection_url" {
  value     = "postgresql://${aws_db_instance.database.username}:${random_password.db_password.result}@${aws_db_instance.database.endpoint}/${aws_db_instance.database.db_name}"
  sensitive = true
}
```

## Multi-Module Hooks

Database hooks can define multiple modules that are all executed for the same resource. Later modules can reference earlier modules' outputs using `module.<name>.<output>` syntax. This enables advanced workflows like database migrations with a write proxy:

```hcl
database {
  when = element(split(":", node.inputs.type), 0) == "postgres"

  module "old_db" {
    build = "./modules/postgres"
    inputs = {
      name = "${environment.name}-${node.component}-${node.name}"
    }
  }

  module "new_db" {
    build = "./modules/postgres"
    inputs = {
      name = "${environment.name}-${node.component}-${node.name}-v2"
    }
  }

  module "write_proxy" {
    build = "./modules/db-write-proxy"
    inputs = {
      primary_url   = module.old_db.url
      secondary_url = module.new_db.url
    }
  }

  outputs = {
    host     = module.old_db.host
    port     = module.old_db.port
    url      = module.old_db.url
    username = module.old_db.username
    password = module.old_db.password
    database = node.name

    read = {
      host = module.old_db.host
      url  = module.old_db.url
    }

    write = {
      host = module.write_proxy.host
      url  = module.write_proxy.url
    }
  }
}
```

In this example:
- `old_db` is provisioned first
- `new_db` is provisioned next
- `write_proxy` references outputs from both `old_db` and `new_db`
- Read traffic goes to the old database, write traffic goes through a dual-write proxy
- Each module's IaC state is tracked separately for independent lifecycle management

## Local Development

For local development datacenters, you might use Docker-based databases:

```hcl
database {
  when = element(split(":", node.inputs.type), 0) == "postgres"

  module "postgres" {
    build = "./modules/docker-postgres"
    inputs = {
      name    = "${node.component}-${node.name}"
      version = try(element(split(":", node.inputs.type), 1), null)
    }
  }

  outputs = {
    host     = "localhost"
    port     = module.postgres.port
    database = module.postgres.database
    url      = module.postgres.connection_url
  }
}
```
